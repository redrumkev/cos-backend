name: Redis Integration Tests

on:
  push:
    branches: [ main, master, develop ]
  pull_request:
    branches: [ main, master, develop ]
  schedule:
    # Run nightly at 2 AM UTC to catch integration regressions
    - cron: '0 2 * * *'

env:
  # Python version
  PYTHON_VERSION: "3.13"
  # UV cache configuration
  UV_CACHE_DIR: ~/.cache/uv
  # Performance test configuration
  REDIS_PERFORMANCE_STRICT: "true"
  BENCHMARK_MIN_ROUNDS: 5
  BENCHMARK_STORAGE_URI: "file://.benchmarks"

jobs:
  redis-integration:
    runs-on: ubuntu-latest

    # Service containers for real Redis testing
    services:
      redis:
        image: redis:7.2-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
          --memory="512m"

      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_USER: cos_user
          POSTGRES_PASSWORD: cos_dev_pass
          POSTGRES_DB: cos_db_dev
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready"
          --health-interval 10s
          --health-retries 5

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        enable-cache: true
        cache-dependency-glob: "uv.lock"

    - name: Cache UV dependencies
      uses: actions/cache@v3
      with:
        path: ${{ env.UV_CACHE_DIR }}
        key: uv-${{ runner.os }}-${{ hashFiles('uv.lock') }}
        restore-keys: |
          uv-${{ runner.os }}-

    - name: Install dependencies
      run: |
        uv pip install --system -e ".[dev]"
        uv pip install --system pytest-benchmark[histogram]

    - name: Verify Redis service
      run: |
        redis-cli -h localhost -p 6379 ping
        redis-cli -h localhost -p 6379 info server | grep redis_version

    - name: Configure test environment
      run: |
        # Redis configuration
        echo "REDIS_HOST=localhost" >> $GITHUB_ENV
        echo "REDIS_PORT=6379" >> $GITHUB_ENV
        echo "REDIS_DB=0" >> $GITHUB_ENV
        echo "REDIS_MAX_CONNECTIONS=20" >> $GITHUB_ENV
        echo "REDIS_URL=redis://localhost:6379" >> $GITHUB_ENV
        echo "REDIS_PASSWORD=" >> $GITHUB_ENV

        # Database configuration (required for imports even if not used)
        echo "DATABASE_URL_DEV=postgresql+asyncpg://cos_user:cos_dev_pass@localhost:5432/cos_db_dev" >> $GITHUB_ENV
        echo "POSTGRES_DEV_URL=postgresql+asyncpg://cos_user:cos_dev_pass@localhost:5432/cos_db_dev" >> $GITHUB_ENV
        echo "NEO4J_URI=bolt://localhost:7687" >> $GITHUB_ENV
        echo "NEO4J_USER=neo4j" >> $GITHUB_ENV
        echo "NEO4J_PASSWORD=test_password" >> $GITHUB_ENV

        # Test configuration
        echo "TESTING=true" >> $GITHUB_ENV
        echo "ENVIRONMENT=test" >> $GITHUB_ENV
        echo "LOG_LEVEL=INFO" >> $GITHUB_ENV
        echo "MEM0_SCHEMA=mem0_cc" >> $GITHUB_ENV

    - name: Run Redis foundation tests
      run: |
        uv run pytest tests/integration/test_redis_foundation.py -v \
          --tb=short \
          --show-capture=no \
          --disable-warnings

    - name: Run Redis resilience tests
      run: |
        uv run pytest tests/integration/test_redis_resilience.py -v \
          --tb=short \
          --show-capture=no \
          --disable-warnings

    - name: Run Redis performance benchmarks
      run: |
        uv run pytest tests/integration/test_redis_performance.py -v \
          --tb=short \
          --show-capture=no \
          --disable-warnings \
          --benchmark-columns=min,max,mean,stddev,median,iqr,outliers,ops,rounds \
          --benchmark-sort=mean \
          --benchmark-json=redis-benchmarks.json \
          --benchmark-save=redis-performance \
          --benchmark-save-data

    - name: Validate performance targets
      run: |
        # Extract benchmark results and validate against targets
        if [ -f redis-benchmarks.json ]; then
          echo "=== Redis Performance Results ==="
          cat redis-benchmarks.json | jq '.benchmarks[] | {name: .name, mean: .stats.mean, min: .stats.min, max: .stats.max}'

          # Check if any benchmark exceeds performance targets
          SLOW_TESTS=$(cat redis-benchmarks.json | jq -r '.benchmarks[] | select(.stats.mean > 0.001) | .name')
          if [ ! -z "$SLOW_TESTS" ]; then
            echo "‚ö†Ô∏è  Warning: Some benchmarks exceed 1ms target:"
            echo "$SLOW_TESTS"
          else
            echo "‚úÖ All benchmarks meet <1ms performance targets"
          fi
        fi

    - name: Test with real Redis containers (testcontainers)
      run: |
        # Run integration tests with testcontainers for production-like testing
        export TESTCONTAINERS_REDIS_IMAGE=redis:7.2-alpine
        uv run pytest tests/integration/ -k "not performance" -v \
          --tb=short \
          --show-capture=no

    - name: Memory leak detection
      run: |
        # Run memory leak tests with detailed tracking
        uv run pytest tests/integration/test_redis_performance.py::TestMemoryLeakDetection -v \
          --tb=short \
          --show-capture=all

    - name: Circuit breaker performance validation
      run: |
        # Validate circuit breaker performance impact
        uv run pytest tests/integration/test_redis_performance.py::TestCircuitBreakerPerformance -v \
          --tb=short \
          --show-capture=all

    - name: Generate performance report
      run: |
        echo "# Redis Performance Test Results" > performance-report.md
        echo "## Test Environment" >> performance-report.md
        echo "- Redis Version: $(redis-cli -h localhost -p 6379 info server | grep redis_version)" >> performance-report.md
        echo "- Python Version: ${{ env.PYTHON_VERSION }}" >> performance-report.md
        echo "- OS: ${{ runner.os }}" >> performance-report.md
        echo "- Date: $(date -u)" >> performance-report.md
        echo "" >> performance-report.md

        if [ -f redis-benchmarks.json ]; then
          echo "## Benchmark Results" >> performance-report.md
          echo "| Test | Mean (ms) | Min (ms) | Max (ms) | Ops/sec |" >> performance-report.md
          echo "|------|-----------|----------|----------|---------|" >> performance-report.md

          cat redis-benchmarks.json | jq -r '.benchmarks[] |
            "| \(.name) | \(.stats.mean * 1000 | floor) | \(.stats.min * 1000 | floor) | \(.stats.max * 1000 | floor) | \(.stats.ops | floor) |"' >> performance-report.md
        fi

    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: redis-benchmarks
        path: |
          redis-benchmarks.json
          performance-report.md
          .benchmarks/
        retention-days: 30

    - name: Upload performance report
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-report
        path: performance-report.md
        retention-days: 7

  # Separate job for testing with different Redis versions
  redis-compatibility:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        redis-version: ["6.2", "7.0", "7.2"]
      fail-fast: false

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        enable-cache: true

    - name: Install dependencies
      run: uv pip install --system -e ".[dev]"

    - name: Start Redis ${{ matrix.redis-version }}
      run: |
        docker run -d --name redis-test \
          -p 6379:6379 \
          redis:${{ matrix.redis-version }}-alpine \
          redis-server --appendonly yes

        # Wait for Redis to be ready
        timeout 30 bash -c 'until docker exec redis-test redis-cli ping; do sleep 1; done'

    - name: Run compatibility tests
      run: |
        export REDIS_HOST=localhost
        export REDIS_PORT=6379
        uv run pytest tests/integration/test_redis_foundation.py -v \
          --tb=short

    - name: Cleanup
      if: always()
      run: |
        docker stop redis-test || true
        docker rm redis-test || true

  # Job for coverage integration and reporting
  coverage-integration:
    runs-on: ubuntu-latest
    needs: redis-integration

    services:
      redis:
        image: redis:7.2-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_USER: cos_user
          POSTGRES_PASSWORD: cos_dev_pass
          POSTGRES_DB: cos_db_dev
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready"
          --health-interval 10s
          --health-retries 5

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install uv
      uses: astral-sh/setup-uv@v3
      with:
        enable-cache: true

    - name: Install dependencies
      run: uv pip install --system -e ".[dev]"

    - name: Run full test suite with coverage
      run: |
        export REDIS_HOST=localhost
        export REDIS_PORT=6379

        # Run all tests with coverage
        uv run pytest \
          --cov=src \
          --cov-report=xml \
          --cov-report=html \
          --cov-report=term-missing \
          --cov-fail-under=79 \
          tests/

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: integration
        name: redis-integration-coverage
        fail_ci_if_error: false

    - name: Upload coverage artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: coverage-report
        path: |
          coverage.xml
          htmlcov/
        retention-days: 7

    - name: Coverage report summary
      run: |
        echo "## Coverage Summary" >> $GITHUB_STEP_SUMMARY
        echo "Target: 79%+ coverage (progressive floor)" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY

        # Extract coverage percentage
        COVERAGE=$(uv run coverage report --format=total)
        echo "Achieved: ${COVERAGE}%" >> $GITHUB_STEP_SUMMARY

        if (( $(echo "$COVERAGE >= 79" | bc -l) )); then
          echo "‚úÖ Coverage target met!" >> $GITHUB_STEP_SUMMARY
        else
          echo "‚ùå Coverage below 79% target" >> $GITHUB_STEP_SUMMARY
        fi

  # Performance regression detection
  performance-regression:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'

    services:
      redis:
        image: redis:7.2-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_USER: cos_user
          POSTGRES_PASSWORD: cos_dev_pass
          POSTGRES_DB: cos_db_dev
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready"
          --health-interval 10s
          --health-retries 5

    steps:
    - name: Checkout PR code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install uv
      uses: astral-sh/setup-uv@v3

    - name: Install dependencies
      run: uv pip install --system -e ".[dev]"

    - name: Run performance benchmarks (PR)
      run: |
        export REDIS_HOST=localhost
        export REDIS_PORT=6379

        uv run pytest tests/integration/test_redis_performance.py \
          --benchmark-json=pr-benchmarks.json \
          --benchmark-only

    - name: Checkout base branch
      uses: actions/checkout@v4
      with:
        ref: ${{ github.base_ref }}

    - name: Run performance benchmarks (base)
      run: |
        uv run pytest tests/integration/test_redis_performance.py \
          --benchmark-json=base-benchmarks.json \
          --benchmark-only

    - name: Compare performance
      run: |
        echo "# Performance Comparison" >> performance-comparison.md
        echo "Comparing PR against base branch (${{ github.base_ref }})" >> performance-comparison.md
        echo "" >> performance-comparison.md

        # Simple comparison - in production, use more sophisticated tools
        if [ -f pr-benchmarks.json ] && [ -f base-benchmarks.json ]; then
          echo "Performance benchmarks completed successfully." >> performance-comparison.md
          echo "Detailed analysis would compare baseline vs PR results." >> performance-comparison.md
        else
          echo "‚ö†Ô∏è Performance benchmark comparison failed" >> performance-comparison.md
        fi

    - name: Comment on PR
      uses: actions/github-script@v6
      if: always()
      with:
        script: |
          const fs = require('fs');

          let comment = '## üöÄ Redis Performance Test Results\n\n';

          try {
            const report = fs.readFileSync('performance-comparison.md', 'utf8');
            comment += report;
          } catch (error) {
            comment += '‚ùå Performance comparison failed to generate.';
          }

          comment += '\n\n---\n*Automated performance testing via GitHub Actions*';

          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });
